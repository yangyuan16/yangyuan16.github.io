I"Ö<h3 id="abstract">Abstract</h3>
<blockquote>
  <p>Notes for reading paper <a href="https://doi.org/10.1016/j.jfineco.2021.08.017">Machine Learning in Chinese Stock Market</a></p>
</blockquote>

<h3 id="main-results">Main results</h3>
<blockquote>
  <p>(1) Neutral networks robustly outperform other methods in terms of 
out-of-sample R^2</p>

  <p>(2) The out-of-sample R^2 are particularly large for the subsamples 
of <font color="red">small firms</font> and non-state-owned firms. Hence,
predictability is more significant for those subsamples of stocks in which <font color="red">retail traders</font> plays a much bigger role.</p>

  <p>(3) Comparing the out-of-sample R^2 with studies in the U.S. market, 
the Chinese market reveals substantially more predictability.</p>

  <p>(4) out-of-sample predictability.</p>

  <p>(5) unconditional and conditional predictive ability test.</p>

  <p>(6) explores whether predictability translates into portfolio gains.</p>

</blockquote>

<h3 id="other-results">other results</h3>
<blockquote>
  <p>We also find that predictability of SOEs (å›½æœ‰ä¼ä¸š) in terms of out-of-sample predictive R^2 
is <font color="red">weaker</font> than for <font color="red">non-SOEs</font> at 
a <font color="red">monthly prediction horizon</font>, which confirms the SOEâ€™s reputation of 
being <font color="red">non-transparent</font> (å›½ä¼çš„å£°èª‰å¹¶ä¸é€æ˜).</p>

  <p>Our results indicate that also that a long-only portfolio can provide substantial and, 
even after including transaction costs, economically signicant performance
(æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œä¸€ä¸ªåªåšå¤šçš„æŠ•èµ„ç»„åˆå¯ä»¥æä¾›å®è´¨æ€§çš„ã€ç”šè‡³åŒ…æ‹¬äº¤æ˜“æˆæœ¬åœ¨å†…çš„ç»æµæ˜¾è‘—çš„ä¸šç»© )</p>
</blockquote>

<h3 id="methods-and-data">Methods and Data</h3>
<h4 id="methods">Methods</h4>
<ul>
  <li>
    <p>(1) Apply the empirical design of Gu et al. (2020b) to the Chinese market.</p>
  </li>
  <li>
    <p>(2) linear models:</p>
    <blockquote>
      <p>ordinary least squares regression (OLS)</p>

      <p>partial least squares (PLS)</p>

      <p>least absolute shrinkage and selection operator (LASSO)</p>

      <p>elastic net (Enet)</p>

      <p>gradient boosted regression trees (GBRT)</p>

      <p>random forest (RF)</p>

      <p>variable subsample aggregation (VASA)</p>

      <p>neutral networks with only 5 layers (NN1 - NN5)</p>
    </blockquote>
  </li>
</ul>

<h4 id="data">Data</h4>
<ul>
  <li>
    <p>(1) WIND</p>
  </li>
  <li>
    <p>(2) CSMAR</p>
  </li>
  <li>
    <p>(3) Predictors: <a href="https://doi.org/10.1093/rfs/hhx019">Green et al. 2017</a>.
The characteristic that provide independent information about average US monthly stock return.
A large collection of stock-level predictive characteristic based on the variable definitions in
the original literature listed in <a href="https://doi.org/10.1093/rfs/hhx019">Green et al. 2017</a>, 
and the literature on China-specific factors.</p>
  </li>
</ul>

<h4 id="data-process">Data process</h4>
<ul>
  <li>(1) classify predictors
    <blockquote>
      <p>90 * 1 vector of stock-level characteristics, 11 * 1 vector of macroeconomic predictors,
80 * 1 vector of dummy variables</p>
    </blockquote>
  </li>
  <li>(2) interaction term between two groups of predictors
    <blockquote>
      <p>Using <font color="red">Kronecker product</font> (ç›´ä¹˜) to represent the interaction terms 
between stock-level characteristics and the eleven macroeconomic predictors:
<img src="http://yangyuan16.github.io//images/posts/Quantitative_analysis/kron_predictors.jpg" alt="" /></p>
    </blockquote>
  </li>
</ul>

<h3 id="empirical-analysis">Empirical analysis</h3>

<ul>
  <li>
    <p>We start exploring our modelsâ€™ prediction performance via out-of-sample 
predictive \(R^2\) and discuss predictability across different subsamples of our data.</p>
  </li>
  <li>Table1:
    <blockquote>
      <p><img src="http://yangyuan16.github.io//images/posts/Quantitative_analysis/ML_ch_table1.jpg" alt="" /></p>
      <ul>
        <li>å°†æ ·æœ¬è¿›è¡Œ SOEs å’Œ non-SOEs åˆ†ç±»çš„åŸå› ï¼š
 As state-owned enterprises (SOEs) play an prominent in Chinaâ€™s capital markets and are
 often criticized for information transparency, we explore the R^2 for both SOEs and 
 non-SOEs.</li>
        <li>å°†æ ·æœ¬è¿›è¡Œ 70% - 30% åˆ†å‰²çš„åŸå› ï¼š<a href="https://doi.org/10.1016/j.jfineco.2019.03.008">As Liu et al. (2019)</a> 
 argue, the samllest 30% of firms often serve as potential shells in reverse 
mergers that circumvent tight IPO constraints. (æ­£å¦‚Liuç­‰äººï¼ˆ2019å¹´ï¼‰æ‰€è¯´ï¼Œæœ€å°çš„30%çš„å…¬å¸é€šå¸¸æ˜¯æ½œåœ¨
çš„è§„é¿ä¸¥æ ¼IPOçº¦æŸçš„åå‘åˆå¹¶ä¸­çš„ç©ºå£³ ) At the same time, Chinese retail investors have a
 notorious preference for investing in small stocks, in particular growth and 
 glamour stocks.(ä¸æ­¤åŒæ—¶ï¼Œä¸­å›½æ•£æˆ·æŠ•èµ„è€…å¯¹å°å‹è‚¡çš„æŠ•èµ„åå¥½å¾ˆå¼ºï¼Œå°¤å…¶æ˜¯å¢é•¿å‹å’Œé­…åŠ›å‹è‚¡ç¥¨ )</li>
        <li>å°†æ ·æœ¬æŒ‰ market capitalization per shareholder åˆ†å‰²çš„åŸå› ï¼šTo shed light on 
the connection between predictability and retail investors, we further conduct
subgroup analysis based on the average market capitalization per shareholder.
At last, we investigate model predictability by looking into the out-of-sample
R^2 for thses two groups.</li>
        <li>OLS model: OLS model achieves a positive R^2 of 0.81%, showing even the simplest model
 still has some predictive power. The R^2 for OLS-3 is slightly lower than that for OLS
 (0.77% v.s. 0.81%), indicating the three covariates alone (size, book-to-market, and
 momentum) are insufficient to account for all predictive power in linear models. <font color="red">It is 
 noteworthy that the OLS model performs much better in China's stock market than 
 in the U.S. stock market </font></li>
        <li>For PLS, LASSO, and Enet, <font color="red"> the improvement of the R^2 directly
 reflect the effectiveness of dimension reduction when we are faced with 
 a large set of covariates </font>. (å¯¹äº PLS, LASSO, Enet, R^2 çš„æå‡åæ˜ äº†é¢å¯¹å¤§é‡åå˜é‡
 æ—¶é™ç»´çš„æœ‰æ•ˆæ€§ã€‚)</li>
        <li>å› æ­¤ï¼ŒR2çš„è¿™ç§æå‡è¡¨æ˜ï¼Œä¸€äº›è‚¡ç¥¨ç‰¹å¾å¯¹äºé¢„æµ‹ä¸­å›½è‚¡å¸‚çš„æœˆåº¦å›æŠ¥æ˜¯å¤šä½™çš„ï¼Œ
 è¿™ä¸<a href="https://doi.org/10.1093/rfs/hhaa009">é¡¾ç­‰äººï¼ˆ2020bï¼‰</a> å¯¹ç¾å›½å¸‚åœºçš„ç ”ç©¶ç»“æœå¾ˆå¥½åœ°å»åˆ.</li>
        <li>æ ‘æ¨¡å‹ã€GBRTå’ŒRFä»¥åŠäº”ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹éƒ½å¯¹ R^2 æœ‰æ‰€æ”¹è¿›,
åœ¨æ‰€æœ‰ä¸ƒç§å‹å·ä¸­ç”šè‡³è¿›ä¸€æ­¥è¾¾åˆ°2%ä»¥ä¸Šã€‚
è¿™ç§æ”¹è¿›è¯æ˜äº†æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨æ•æ‰é¢„æµ‹<font color="red">å› å­ä¹‹é—´å¤æ‚äº¤äº’æ–¹é¢çš„ä¼˜åŠ¿</font>ï¼Œ<a href="https://doi.org/10.1093/rfs/hhaa009">Guç­‰äººï¼ˆ2020bï¼‰</a> ä¹Ÿå¼ºè°ƒäº†è¿™ä¸€ç‚¹ã€‚</li>
        <li>The results in Table 1 suggest that all models have a much better predictive performance
for small stocks. The linear models, the linear models, OLS and OLS-3, now
raise their R^2 to above 1%, while the regularized linear model, including PLS, 
LASSO, and Enet, nearly double their performance.</li>
        <li>è´Ÿçš„ R^2: Interestingly, OLS, RF, and even GBRT, now have negative R^2, indicating
they are easily dominated by <font color="red">a naive forecast of zero returns</font>
for all stocks in all periods.</li>
      </ul>
    </blockquote>
  </li>
  <li>Table2:
    <blockquote>
      <p><img src="http://yangyuan16.github.io//images/posts/Quantitative_analysis/ML_ch_table2.jpg" alt="" /></p>
      <ul>
        <li>è¡¨æ˜æœºå™¨å­¦ä¹ æ–¹æ³•å¯ä»¥åœ¨æ›´é•¿çš„æ—¶é—´å†…æˆåŠŸåœ°éš”ç¦»æŒç»­é£é™©æº¢ä»·: We find that the annual
out-of-sample R^2 are higher than their monthly counterparts, indicating machine learning
methods can successfully isolate persistent risk premiums at longer horizons.<br />
*æˆ‘ä»¬å°†çŸ­æœŸå¯é¢„æµ‹æ€§ï¼ˆå°¤å…¶æ˜¯å°å‹è‚¡ï¼‰å½’å› äºæ•£æˆ·æŠ•èµ„è€…åœ¨ä¸­å›½è‚¡å¸‚ä¸­çš„çªå‡ºä½œç”¨ã€‚
æ­£å¦‚æˆ‘ä»¬ç¨åå°†çœ‹åˆ°çš„ï¼Œåœ¨è¾ƒçŸ­çš„è§†é‡å†…ï¼Œç¥ç»ç½‘ç»œæ›´åŠ é‡è§†å°ç›˜è‚¡çš„æ³¢åŠ¨æ€§å’ŒåŠ¨é‡ç›¸å…³å˜é‡ï¼Œ
è¿™å¯èƒ½åæ˜ äº†æ•£æˆ·æŠ•èµ„è€…çš„çŸ­æœŸæŠ•æœºè¡Œä¸ºï¼Œä»¥åŠä»–ä»¬çš„çŸ¥ååº¦ä¹Ÿå°±æ˜¯å–œæ¬¢ä¼˜å…ˆäº¤æ˜“å°å‹è‚¡</li>
      </ul>
    </blockquote>
  </li>
</ul>

:ET